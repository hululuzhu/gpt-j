{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "GPT-J-6B_Iference_demo_and_chinese_coding_examples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60f26dc6df4f465cb1b7e6f3dbb26a7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3ea0ac993d5b48bfbf45a09c13372df1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_58dad8648bda400ab99750996464bf8b",
              "IPY_MODEL_e47d07edb7614823b17a4990f61dbaf7"
            ]
          }
        },
        "3ea0ac993d5b48bfbf45a09c13372df1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58dad8648bda400ab99750996464bf8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b638c680d95247efa080f5b3f65a32f0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_79a78352b80d410e995b8432abf52aca"
          }
        },
        "e47d07edb7614823b17a4990f61dbaf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca3c30ad8d1a49349094553c856a94ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:01&lt;00:00, 645kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_afd217517e1d45fba4bbf6a8d0eca0f2"
          }
        },
        "b638c680d95247efa080f5b3f65a32f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "79a78352b80d410e995b8432abf52aca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca3c30ad8d1a49349094553c856a94ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "afd217517e1d45fba4bbf6a8d0eca0f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f204517f764d4aae9de4a64d7036ced2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c3aa4df4c2284e0ea7e4ace509d633b1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f10d6a20b6d4c50af6ddb4f1ca6c5f2",
              "IPY_MODEL_e64e6047fa684a8d8115a810914ed212"
            ]
          }
        },
        "c3aa4df4c2284e0ea7e4ace509d633b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f10d6a20b6d4c50af6ddb4f1ca6c5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_178c32d1da9742f39a5aafa9bc468897",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17adc92d70c94e31a533b4f6225c6d07"
          }
        },
        "e64e6047fa684a8d8115a810914ed212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ad0c3214a174ebbad8c8b8ff70a82db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 524kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b25bb4fcfa594e59a33ec19338836d01"
          }
        },
        "178c32d1da9742f39a5aafa9bc468897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17adc92d70c94e31a533b4f6225c6d07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ad0c3214a174ebbad8c8b8ff70a82db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b25bb4fcfa594e59a33ec19338836d01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba521fffc8b0474f9fa21572fdfdb43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8210ad1e12744282b3ab7548ae164d55",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a9d243ab318b4b22b03f94892e9b3b72",
              "IPY_MODEL_9f250c0a49184267b9b4a55a0de643c9"
            ]
          }
        },
        "8210ad1e12744282b3ab7548ae164d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9d243ab318b4b22b03f94892e9b3b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1908fb44112e40ea9cfe90ce450a770e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355256,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355256,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d68610cdec8945a6a738152cc11138c7"
          }
        },
        "9f250c0a49184267b9b4a55a0de643c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e9f4aab427f4ec9b60a048a0d409bb2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 2.69MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ed66b98ca834e3097ffc79423435691"
          }
        },
        "1908fb44112e40ea9cfe90ce450a770e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d68610cdec8945a6a738152cc11138c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e9f4aab427f4ec9b60a048a0d409bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ed66b98ca834e3097ffc79423435691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "973dc86d134548738e316fe032fca6b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a717eec159714d68a6c22f03629deef0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e67c224720664309bcfbfb69b71ad580",
              "IPY_MODEL_faeb5a78fec441938531d3cac6133366"
            ]
          }
        },
        "a717eec159714d68a6c22f03629deef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e67c224720664309bcfbfb69b71ad580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_576159f502f2489b9481ded4c24eacc7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8dd8d9264474d21b6703c13fb08685f"
          }
        },
        "faeb5a78fec441938531d3cac6133366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29c538499b914c8ba8b5504e52b32e86",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:00&lt;00:00, 21.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a38382df719343c1bceae80acf68bad4"
          }
        },
        "576159f502f2489b9481ded4c24eacc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8dd8d9264474d21b6703c13fb08685f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29c538499b914c8ba8b5504e52b32e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a38382df719343c1bceae80acf68bad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/gpt-j/blob/main/GPT_J_6B_Iference_demo_and_chinese_coding_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a branch of [kingoflolz's GPT-J colab](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb)\n",
        "\n",
        "- Modified a few places for "
      ],
      "metadata": {
        "id": "LaMprSh87836"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-J-6B Inference Demo\n",
        "\n",
        "<a href=\"http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook demonstrates how to run the [GPT-J-6B model](https://github.com/kingoflolz/mesh-transformer-jax/#GPT-J-6B). See the link for more details about the model, including evaluation metrics and credits."
      ],
      "metadata": {
        "id": "pHIJVqHsh4An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "\n",
        "First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load).\n",
        "\n",
        "!!! **Make sure you are using a TPU runtime!** !!!"
      ],
      "metadata": {
        "id": "8CMw_dSQKfhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd -q"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "real\t1m46.693s\n",
            "user\t0m5.344s\n",
            "sys\t0m23.934s\n"
          ]
        }
      ],
      "metadata": {
        "id": "zzPDt-r4CL4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b7302e-614d-41cc-fa2f-12fab5fa8f75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!apt install zstd -q\r\n",
        "\r\n",
        "# the \"slim\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\r\n",
        "# !time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\r\n",
        "\r\n",
        "!time tar -I zstd -xf step_383500_slim.tar.zstd\r\n",
        "\r\n",
        "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\r\n",
        "!pip install -r -q mesh-transformer-jax/requirements.txt\r\n",
        " \r\n",
        "# jax 0.2.12 is required due to a regression with xmap in 0.2.13\r\n",
        "!pip install -q mesh-transformer-jax/ jax==0.2.12\r\n",
        "\r\n",
        "# Looks necessary to avoid the tokenizer error below. Seem imcompatibility between transformer lib and TF keras\r\n",
        "!pip uninstall -y tensorflow\r\n",
        "!pip install tensorflow==2.3.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  zstd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 278 kB of archives.\n",
            "After this operation, 1,141 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 zstd amd64 1.3.3+dfsg-2ubuntu1.2 [278 kB]\n",
            "Fetched 278 kB in 1s (375 kB/s)\n",
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.3.3+dfsg-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking zstd (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "Setting up zstd (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\n",
            "real\t1m29.566s\n",
            "user\t0m29.950s\n",
            "sys\t0m26.215s\n",
            "Cloning into 'mesh-transformer-jax'...\n",
            "remote: Enumerating objects: 668, done.\u001b[K\n",
            "remote: Counting objects: 100% (276/276), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 668 (delta 235), reused 190 (delta 190), pack-reused 392\u001b[K\n",
            "Receiving objects: 100% (668/668), 182.00 KiB | 3.64 MiB/s, done.\n",
            "Resolving deltas: 100% (442/442), done.\n",
            "\u001b[31mERROR: Invalid requirement: 'mesh-transformer-jax/requirements.txt'\n",
            "Hint: It looks like a path. The path does exist. The argument you provided (mesh-transformer-jax/requirements.txt) appears to be a requirements file. If that is the case, use the '-r' flag to install the packages specified within it.\u001b[0m\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 590 kB 5.2 MB/s \n",
            "\u001b[?25h  Building wheel for mesh-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7xAFw-LOYfe",
        "outputId": "40d900c9-8edf-46dd-e5be-5dc505285c2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Model\n"
      ],
      "metadata": {
        "id": "aO1UXepF-0Uq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import requests \r\n",
        "from jax.config import config\r\n",
        "\r\n",
        "colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\r\n",
        "url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\r\n",
        "requests.post(url)\r\n",
        "\r\n",
        "# The following is required to use TPU Driver as JAX's backend.\r\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\r\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "outputs": [],
      "metadata": {
        "id": "ex0qJgaueZtJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q optax\r\n",
        "!pip install -q transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 70.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 58.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vHINc8YRyAW",
        "outputId": "b8ffa999-ece0-4180-dc89-b1168fddc292"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Added as of 08/16\r\n",
        "!pip install -q dm-haiku \r\n",
        "!pip install -q einops\r\n",
        "!pip install -q ray"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 284 kB 5.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pha_ltz3R8ur",
        "outputId": "aaf56ec6-823e-473d-b09b-cf152bf3a731"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes the next step errors for some reason, just run it again ¯\\\\\\_(ツ)\\_/¯"
      ],
      "metadata": {
        "id": "NIgUVdFLe4A8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import time\r\n",
        "\r\n",
        "import jax\r\n",
        "from jax.experimental import maps\r\n",
        "import numpy as np\r\n",
        "import optax\r\n",
        "import transformers\r\n",
        "\r\n",
        "from mesh_transformer.checkpoint import read_ckpt\r\n",
        "from mesh_transformer.sampling import nucleaus_sample\r\n",
        "from mesh_transformer.transformer_shard import CausalTransformer"
      ],
      "outputs": [],
      "metadata": {
        "id": "-A5IGYSaeze3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "params = {\r\n",
        "  \"layers\": 28,\r\n",
        "  \"d_model\": 4096,\r\n",
        "  \"n_heads\": 16,\r\n",
        "  \"n_vocab\": 50400,\r\n",
        "  \"norm\": \"layernorm\",\r\n",
        "  \"pe\": \"rotary\",\r\n",
        "  \"pe_rotary_dims\": 64,\r\n",
        "\r\n",
        "  \"seq\": 2048,\r\n",
        "  \"cores_per_replica\": 8,\r\n",
        "  \"per_replica_batch\": 1,\r\n",
        "}\r\n",
        "\r\n",
        "per_replica_batch = params[\"per_replica_batch\"]\r\n",
        "cores_per_replica = params[\"cores_per_replica\"]\r\n",
        "seq = params[\"seq\"]\r\n",
        "\r\n",
        "\r\n",
        "params[\"sampler\"] = nucleaus_sample\r\n",
        "\r\n",
        "# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\r\n",
        "params[\"optimizer\"] = optax.scale(0)\r\n",
        "\r\n",
        "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\r\n",
        "devices = np.array(jax.devices()).reshape(mesh_shape)\r\n",
        "\r\n",
        "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\r\n",
        "\r\n",
        "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60f26dc6df4f465cb1b7e6f3dbb26a7b",
              "version_minor": 0,
              "version_major": 2
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f204517f764d4aae9de4a64d7036ced2",
              "version_minor": 0,
              "version_major": 2
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba521fffc8b0474f9fa21572fdfdb43a",
              "version_minor": 0,
              "version_major": 2
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "973dc86d134548738e316fe032fca6b8",
              "version_minor": 0,
              "version_major": 2
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "QAgKq-X2kmba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "60f26dc6df4f465cb1b7e6f3dbb26a7b",
            "3ea0ac993d5b48bfbf45a09c13372df1",
            "58dad8648bda400ab99750996464bf8b",
            "e47d07edb7614823b17a4990f61dbaf7",
            "b638c680d95247efa080f5b3f65a32f0",
            "79a78352b80d410e995b8432abf52aca",
            "ca3c30ad8d1a49349094553c856a94ae",
            "afd217517e1d45fba4bbf6a8d0eca0f2",
            "f204517f764d4aae9de4a64d7036ced2",
            "c3aa4df4c2284e0ea7e4ace509d633b1",
            "2f10d6a20b6d4c50af6ddb4f1ca6c5f2",
            "e64e6047fa684a8d8115a810914ed212",
            "178c32d1da9742f39a5aafa9bc468897",
            "17adc92d70c94e31a533b4f6225c6d07",
            "9ad0c3214a174ebbad8c8b8ff70a82db",
            "b25bb4fcfa594e59a33ec19338836d01",
            "ba521fffc8b0474f9fa21572fdfdb43a",
            "8210ad1e12744282b3ab7548ae164d55",
            "a9d243ab318b4b22b03f94892e9b3b72",
            "9f250c0a49184267b9b4a55a0de643c9",
            "1908fb44112e40ea9cfe90ce450a770e",
            "d68610cdec8945a6a738152cc11138c7",
            "0e9f4aab427f4ec9b60a048a0d409bb2",
            "3ed66b98ca834e3097ffc79423435691",
            "973dc86d134548738e316fe032fca6b8",
            "a717eec159714d68a6c22f03629deef0",
            "e67c224720664309bcfbfb69b71ad580",
            "faeb5a78fec441938531d3cac6133366",
            "576159f502f2489b9481ded4c24eacc7",
            "c8dd8d9264474d21b6703c13fb08685f",
            "29c538499b914c8ba8b5504e52b32e86",
            "a38382df719343c1bceae80acf68bad4"
          ]
        },
        "outputId": "76c01ef2-8234-442a-bb45-1dd74f5d1e99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create the network and load the parameters from the downloaded files. Expect this to take around 5 minutes."
      ],
      "metadata": {
        "id": "yFgRkUgfiNdA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\r\n",
        "\r\n",
        "network = CausalTransformer(params)\r\n",
        "\r\n",
        "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\r\n",
        "\r\n",
        "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/maps.py:412: UserWarning: xmap is an experimental feature and probably has bugs!\n",
            "  warn(\"xmap is an experimental feature and probably has bugs!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key shape (8, 2)\n",
            "in shape (1, 2048)\n",
            "dp 1\n",
            "mp 8\n",
            "Total parameters: 6053381344\n",
            "read from disk/gcs in 16.6356s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwNETD2Uk8nu",
        "outputId": "76c34bdf-5cce-4ded-ab3f-9eecc9ea1805"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Model\n",
        "\n",
        "Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.\n",
        "\n",
        "Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when changed).\n",
        "\n",
        "You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.\n",
        "\n",
        "*Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training.*"
      ],
      "metadata": {
        "id": "A-eT7Sw6if4J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# allow text wrapping in generated output: https://stackoverflow.com/a/61401455\r\n",
        "from IPython.display import HTML, display\r\n",
        "\r\n",
        "def set_css():\r\n",
        "  display(HTML('''\r\n",
        "  <style>\r\n",
        "    pre {\r\n",
        "        white-space: pre-wrap;\r\n",
        "    }\r\n",
        "  </style>\r\n",
        "  '''))\r\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "outputs": [],
      "metadata": {
        "id": "TrNqO2qyUqDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def infer(context, top_p=0, temp=1.0, gen_len=512):\r\n",
        "    tokens = tokenizer.encode(context)\r\n",
        "\r\n",
        "    provided_ctx = len(tokens)\r\n",
        "    pad_amount = seq - provided_ctx\r\n",
        "\r\n",
        "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\r\n",
        "    batched_tokens = np.array([padded_tokens] * total_batch)\r\n",
        "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\r\n",
        "\r\n",
        "    start = time.time()\r\n",
        "    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\r\n",
        "\r\n",
        "    samples = []\r\n",
        "    decoded_tokens = output[1][0]\r\n",
        "\r\n",
        "    for o in decoded_tokens[:, :, 0]:\r\n",
        "      samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\r\n",
        "\r\n",
        "    # print(f\"completion done in {time.time() - start:06}s\")\r\n",
        "    return samples\r\n",
        "\r\n",
        "# print(infer(\"EleutherAI is\")[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZVzs2TYlvYeX",
        "outputId": "ff04a2d1-23b7-409f-fbb2-00ca939595cc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title  { form-width: \"300px\" }\r\n",
        "top_p = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\r\n",
        "temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\r\n",
        "\r\n",
        "context = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\"\"\r\n",
        "\r\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=512, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 13.511082410812378s\n",
            "\u001b[1mIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\u001b[0m Scientists, studying the conversation, suspected that there was no rational explanation for the situation. As a precaution, it was decided that another team must be sent to go and check up on the herd.\n",
            "\n",
            "Castlevania: Lords Of Shadow - Dawn Of Destiny\n",
            "\n",
            "Homicide : The Corpse Wasn't There\n",
            "\n",
            "An unspeakable pair of evil fields proceeded to face off against each other: the Crepidius. The goal? To find out (or keep them from figuring it out in the first place). Since a fellow scientist had recently gone missing, two scientists on vacation had been sent to research. As each whole was set loose, a terrible pandemonium ensued. And why not - the Crepidius were completely aware that in a place where people could shoot fireballs at it, it was completely superfluous to try and hide. And, of course, they weren't aware of what the other creatures were capable of...\n",
            "\n",
            "Tropes Used: Riddle Of Steel* Chess: While there was a Chessboard in ''Creator/{{Vatican}}'', it was only half-finished, and it was a Red Herring for this puzzle. Also, repeatedly falling into traps is always amusing.* WrongRedPill: An illustration in the Collector's Edition shows a music box you can use. But, instead of playing \"I can see a li'l silver lining\" from 1964 by TheMisunderstoodVixen, it's a cheesy pop music number by 1978's [[DaveEverett Angelo Pace]]. You assume it's because the box is crap, but it's actually just FateBeingAwesome.** What kicks off this train is the very first box attacked. It's an exact copy of it except the \"123123\" and \"478956\" are reversed, rendering the real message unrecoverable. Note that it signifies that the Illuminati's leader never mentioned his secretly superior rival, therefore reasons he'd be an enemy? Apparently.* Another BigBad: According to Atanu:* ThatScarTouchedYourHeart: Reprise. To crushing music. \"It's a Loneliness Engine......\"* BladeMovie: From the enigmatic teaser video.* ComboHelterSkelter: While not explicitly shown, the first topic of conversation in the opening event is that a god of the same class as Dracule is up and running. In an open conversation among Alico, Ana and Nikolai, when Ana mentions being kidnapped in \"Alto Caballero\", [[FreezeFrameBonus the action\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "nvlAK6RbCJYg",
        "outputId": "1b2d6054-171b-4b21-a4c2-3724aeb225d3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# context = \"\"\"public String get_first_char(String input_str) {\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "context = \"\"\"def get_first_char(String input_str):\r\n",
        "\"\"\"\r\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=64, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 41.18787169456482s\n",
            "\u001b[1mdef get_first_char(String input_str):\n",
            "\u001b[0m    return safe_get_first_char(input_str, character_set=\"utf-8\")\n",
            "\n",
            "stable_input_str = your_new_string\n",
            "\n",
            "encoded_now = str(encode(stable_input_str, \"ascii\", errors=\"strict\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "lJ0V0oSKJbbY",
        "outputId": "075ea122-3d1b-4fbf-8dc9-70068595db59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context=\"\"\"# function to check string is\r\n",
        "# palindrome or not\r\n",
        "def isPalindrome(s):\r\n",
        "\"\"\"\r\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=64, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 1.9251036643981934s\n",
            "\u001b[1m# function to check string is\n",
            "# palindrome or not\n",
            "def isPalindrome(s):\n",
            "\u001b[0m    s=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" # length of given string.\n",
            "    if len(s)%2==1:\n",
            "        k=len(s)\n",
            "    else:\n",
            "  \n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "DppCOegOLBze",
        "outputId": "ac57f13e-177a-46ad-966b-51a9c122f848"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context=\"\"\"\"We make a living by what we get, but we make a life by what we give,\" is one of my favorite quotes by the late Winston Churchill. I gave six hours to a non-profit charity this past week by selling t-shirts to promote breast cancer awareness and it was most fulfilling. Seeing my hard work make a difference inspired me to provide services for the local Minot Human Society as well. In the next passages I will explain my tasks, personal thoughts on community service conducted and the correlation between the chapter lesson and my community efforts. .\"\"\"\r\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=512, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 13.540693759918213s\n",
            "\u001b[1m\"We make a living by what we get, but we make a life by what we give,\" is one of my favorite quotes by the late Winston Churchill. I gave six hours to a non-profit charity this past week by selling t-shirts to promote breast cancer awareness and it was most fulfilling. Seeing my hard work make a difference inspired me to provide services for the local Minot Human Society as well. In the next passages I will explain my tasks, personal thoughts on community service conducted and the correlation between the chapter lesson and my community efforts. .\u001b[0m\n",
            "\n",
            "Lesson #36: Promoting Breast Cancer Awareness\n",
            "\n",
            "I found three locations that would be easiest to get to and provide the best opportunity to sell t-shirts. While waiting for the boys, I studied classes for the Breast Cancer Foundation “You're Never to Young to Get Breast Cancer” course, for Cancer Information Services “ How to Identify Breast Cancer in Your Friends and Family” and for NEThealth “How to Become A Well-informed Health Care Consumer”. These courses provide the opportunity to discuss and learn more about Breast Cancer development. It is a life-threatening condition that has a direct effect on their families and is the 2nd leading cancer killer in American women. Most people who engage in breast cancer activity are there because somebody was diagnosed with Breast Cancer and they now want you to know more about the condition and step in to help.\n",
            "\n",
            "The first location really only consists of a “sign” that would make it easy enough for me to get to after school; however it pertains to the foundation. I looked up some web resources to answer my questions about Breast Cancer. There are very few known causes of breast cancer and screening is the only way to read your risk information. The more annual mammograms you have the less likely you are to develop cancer. When you do receive a mammography the Anti-cancer Device is the Computed Tomography or Mammography (CT or MRI, respectively) that scans the Mammoscope. Some health professionals recommend women have regular mammograms starting at age 40 but only one in five women has ever had a mammogram. Today some doctors sugjest annual mammograms starting at age 50 and by the time a woman is 80 years of age, only one in 100 has ever had a mammogram.\n",
            "Among my personal thoughts, I would like to offer ideas to other women. I feel you are never to young. Friends just starting out with children soon are in your position. You may have many growing needs that require new lessons and new experiences. Often you may even be seeking new things to learn, one of your passions and find use in dealing with this issue. Learning more about Breast Cancer awareness can be very comforting to know that you will not suddenly notice that some of your close friends or family members have had baldness or unexplained symptoms. Your peers may at your suggestion take the course.\n",
            "\n",
            "The second location can be reached within the city limits requiring drivers, but a short walk. I needed to restock the only art supply\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "ulzB2nBLLeu5",
        "outputId": "a6351a31-e9cf-4a53-fcad-e72d724deca0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chinese Inference Examples"
      ],
      "metadata": {
        "id": "jvo8qmVv8mnB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context = \"\"\"问题：如何用python或者java计算三个数中最大的那个数？\n",
        "答案:\"\"\""
      ],
      "outputs": [],
      "metadata": {
        "id": "TFLEGmOoZ_bz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"question: how to write a python program to get the largest of 3 integer numbers?\n",
        "answer:\"\"\"\n",
        "print(infer(gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mquestion: how to write a python program to get the largest of 3 integer numbers?\n",
            "answer:\u001b[0m\n",
            "def largest(a, b, c):\n",
            "    if a > b:\n",
            "        if a > c:\n",
            "            return a\n",
            "        else:\n",
            "            return b\n",
            "    else:\n",
            "        if b > c:\n",
            "            return b\n",
            "        else:\n",
            "            return c\n",
            "\n",
            "CPU times: user 559 ms, sys: 2.92 s, total: 3.48 s\n",
            "Wall time: 3.59 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "65662YP2hAhU",
        "outputId": "a15a7e2c-05a3-4e45-bc3b-212c261c32e5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：如何能用python计算三个整数中最大的那个数？\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：如何能用python计算三个整数中最大的那个数？\n",
            "答案:\u001b[0m\n",
            "import math\n",
            "\n",
            "def max_num(a, b, c):\n",
            "    max_num = max(a, b, c)\n",
            "    return max_num\n",
            "\n",
            "print(max_num(1, 2, 3))\n",
            "\n",
            "# 输出:\n",
            "# 3\n",
            "\n",
            "# 参考：\n",
            "# https://www.zhihu.com/question/27058981/answer/27058981\n",
            "\n",
            "# 参考：\n",
            "# https://www.zhihu.com/question/27\n",
            "CPU times: user 711 ms, sys: 2.4 s, total: 3.12 s\n",
            "Wall time: 3.59 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "gJa6yVhCMc1T",
        "outputId": "ac5ae019-84ac-47ed-a55a-6b407eac8dd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"question: which programming language is the best?\n",
        "answer:\"\"\"\n",
        "print(infer(gen_len=8, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mquestion: which programming language is the best?\n",
            "answer:\u001b[0m it depends\n",
            "\n",
            "I'm a programmer\n",
            "CPU times: user 170 ms, sys: 330 ms, total: 500 ms\n",
            "Wall time: 480 ms\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "Bog2NDrOhzF7",
        "outputId": "46506d72-a69d-4e28-b786-9a2a71b070f5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：哪个编程语言是最好的编程语言？\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=64, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：哪个编程语言是最好的编程语言？\n",
            "答案:\u001b[0m C++\n",
            "\n",
            "### 关于编程语言的更多细节\n",
            "\n",
            "#### 关于编程语言的更多细节\n",
            "CPU times: user 604 ms, sys: 1.28 s, total: 1.88 s\n",
            "Wall time: 1.97 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "8h8JzpKahnTe",
        "outputId": "05669eae-1d0d-4699-9805-0473c14f6dc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：php是世界上最好的编程语言吗？\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=64, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：php是世界上最好的编程语言吗？\n",
            "答案:\u001b[0m 嗯，不是。\n",
            "\n",
            "## 关于PHP的编程语言\n",
            "\n",
            "PHP是一个编程语言，它的编程语\n",
            "CPU times: user 399 ms, sys: 1.42 s, total: 1.81 s\n",
            "Wall time: 1.94 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "gDwOcbQSiJY3",
        "outputId": "bc9f597f-41c3-40f4-89ae-646d733bfc3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：写一个java程序从3个输入数字中获取第二大整数\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：写一个java程序从3个输入数字中获取第二大整数\n",
            "答案:\u001b[0m\n",
            "\n",
            "```java\n",
            "public class Solution {\n",
            "    public int getSecondLargest(int[] nums) {\n",
            "        int max = nums[0];\n",
            "        for (int i = 1; i < nums.length; i++) {\n",
            "            if (nums[i] > max) {\n",
            "                max = nums[i];\n",
            "            }\n",
            "CPU times: user 696 ms, sys: 2.71 s, total: 3.4 s\n",
            "Wall time: 3.58 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "VexrtR4HegQY",
        "outputId": "5568ceb8-62a8-42ef-f760-e8023e2ee15f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：写一个shell程序从输入txt文件中读取最长的string\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：写一个shell程序从输入txt文件中读取最长的string\n",
            "答案:\u001b[0m\n",
            "#!/bin/bash\n",
            "\n",
            "while read -r line\n",
            "do\n",
            "        if [[ $line =~ ^[a-zA-Z0-9_]+$ ]]\n",
            "        then\n",
            "                echo $line\n",
            "        fi\n",
            "done < $1\n",
            "\n",
            "参考：\n",
            "\n",
            "https://www.cnblogs.com/jiajun-liu/p/8454571.html\n",
            "https://www.cn\n",
            "CPU times: user 601 ms, sys: 2.55 s, total: 3.15 s\n",
            "Wall time: 3.58 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "zS40wS6wfBV8",
        "outputId": "3000cb6d-d7f6-424e-f3ff-7b5306ee0dbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：写一个css实现渐变紫红色圆角\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：写一个css实现渐变紫红色圆角\n",
            "答案:\u001b[0m\n",
            "\n",
            ".container {\r\n",
            "  width: 100%;\r\n",
            "  height: 100%;\r\n",
            "  background: #f00;\r\n",
            "  overflow: hidden;\r\n",
            "}\r\n",
            "\r\n",
            ".container:before {\r\n",
            "  content: \"\";\r\n",
            "  position: absolute;\r\n",
            "  top: 0;\r\n",
            "  left: 0;\r\n",
            "  right: 0;\r\n",
            "  bottom: 0;\r\n",
            "  background: #f00;\r\n",
            "  transform: skew(20deg);\r\n",
            "  transform-origin: top left;\r\n",
            "}\n",
            "<div class=\"container\">\n",
            "CPU times: user 680 ms, sys: 2.57 s, total: 3.25 s\n",
            "Wall time: 3.58 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "7pvQVu3_fQbS",
        "outputId": "850086c9-2a74-4c50-ad1b-73fb2bcf0984"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：写一个html程序，页面上有“欢迎你”三个大字\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：写一个html程序，页面上有“欢迎你”三个大字\n",
            "答案:\u001b[0m\n",
            "\n",
            "```html\n",
            "<!DOCTYPE html>\n",
            "<html>\n",
            "<head>\n",
            "    <title>欢迎你</title>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>欢迎你</h1>\n",
            "</body>\n",
            "</html>\n",
            "```\n",
            "\n",
            "### 关于编程\n",
            "\n",
            "- [编程的经验](https://www.zhihu.com/question/20480072)\n",
            "- [\n",
            "CPU times: user 623 ms, sys: 2.74 s, total: 3.36 s\n",
            "Wall time: 3.58 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "xeXBPU1FftB7",
        "outputId": "a3538d27-7f04-414a-e566-af77197154cb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"\n",
        "问题：写一个SQL来查找data101数据表中名字（name）是“小凡”的所有8月份在上海的消费记录\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=64, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "问题：帮我写一个SQL来查找data101数据表中名字（name）是“小凡”的所有8月份在上海的消费记录\n",
            "答案:\u001b[0m\n",
            "SELECT * FROM data101 WHERE name='小凡' AND month='8' AND city='上海'\n",
            "\n",
            "问题：帮我写一个SQL来查找data101数据表中�\n",
            "CPU times: user 405 ms, sys: 1.39 s, total: 1.8 s\n",
            "Wall time: 1.93 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "hMZNysK2gJjw",
        "outputId": "cce8e341-f5f2-4e54-bff6-60e29f99074c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"\n",
        "问题：写一个网络爬虫去查找网络上举报“小凡”的热心公众号\n",
        "答案:\"\"\"\n",
        "print(infer(gen_len=256, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "问题：写一个网络爬虫去查找网络上举报“小凡”的热心公众号\n",
            "答案:\u001b[0m\n",
            "\n",
            "```python\n",
            "import requests\n",
            "import json\n",
            "import time\n",
            "\n",
            "def get_hot_posts(url):\n",
            "    r = requests.get(url)\n",
            "    posts = []\n",
            "    for i in r.json():\n",
            "        posts.append(i)\n",
            "    return posts\n",
            "\n",
            "def get_hot_posts_by_user(user_id):\n",
            "    url = 'https://www.weibo.com/' + user_id + '/hot_posts'\n",
            "    r = requests.get(url)\n",
            "    posts = []\n",
            "    for i in r.json():\n",
            "        posts.append(i)\n",
            "    return posts\n",
            "\n",
            "def get_hot_posts_by_user_and_time(user_id, time):\n",
            "    url = 'https://www.weibo.com/' + user_id + '/hot_posts'\n",
            "    r = requests.get(url)\n",
            "    posts = []\n",
            "    for i in r.json():\n",
            "      \n",
            "CPU times: user 1.22 s, sys: 4.34 s, total: 5.56 s\n",
            "Wall time: 6.91 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "n0XweCwakpUd",
        "outputId": "83020ecc-6be7-46d5-db81-d8be806155ff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"Questions: How to find the largest of 5 numbers with python or java?\n",
        "Answer:\"\"\"\n",
        "# print(infer(top_p=0, temp=0, gen_len=128, context=context)[0])\n",
        "print(infer(top_p=0, temp=0, gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mQuestions: How to find the largest of 5 numbers with python or java?\n",
            "Answer:\u001b[0m\n",
            "import math\n",
            "\n",
            "def largest(numbers):\n",
            "    max_number = numbers[0]\n",
            "    for number in numbers:\n",
            "        if number > max_number:\n",
            "            max_number = number\n",
            "    return max_number\n",
            "\n",
            "print(largest([1, 2, 3, 4, 5]))\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the max() function.\n",
            ">>> max(1, 2, 3, 4, 5)\n",
            "5\n",
            "\n",
            "A:\n",
            "\n",
            "You can use\n",
            "CPU times: user 745 ms, sys: 2.25 s, total: 2.99 s\n",
            "Wall time: 3.58 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "5CS1nBUxNrUu",
        "outputId": "040ecae0-46a7-44b3-d05d-7cae290f54d3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# %%time\n",
        "context = \"\"\"\n",
        "问题：小刚是小明的朋友，小刚还是小花的朋友。小刚的朋友是谁？\n",
        "答案：小明和小花。\n",
        "\n",
        "问题：小刚是小明的朋友，小明不喜欢小花。小花喜欢谁？\n",
        "答案：可能是小刚，也可能是小明。\n",
        "\n",
        "问题：小凡喜欢在外面鬼混。小花还很喜欢小凡。谁是坏人？\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=4, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "问题：小刚是小明的朋友，小刚还是小花的朋友。小刚的朋友是谁？\n",
            "答案：小明和小花。\n",
            "\n",
            "问题：小刚是小明的朋友，小明不喜欢小花。小花喜欢谁？\n",
            "答案：可能是小刚，也可能是小明。\n",
            "\n",
            "问题：小凡喜欢在外面鬼混。小花还很喜欢小凡。谁是坏人？\n",
            "答案：\u001b[0m小花\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "rswM8Hvbb5Wl",
        "outputId": "c1e2ed54-7731-4890-8d17-da571dba6491"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：小刚是小明的朋友，小明不喜欢小花。小花喜欢谁？\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=20, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：小刚是小明的朋友，小明不喜欢小花。小花喜欢谁？\n",
            "答案：\u001b[0m小花喜欢小刚。\n",
            "\n",
            "English: \n",
            "CPU times: user 700 ms, sys: 373 ms, total: 1.07 s\n",
            "Wall time: 914 ms\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "4xGzq4UqaDNO",
        "outputId": "4555e083-c595-42f1-f637-a51467df4c07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：小刚是小明的朋友，小明不喜欢小花。小花喜欢谁？\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=20, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 0.7992494106292725s\n",
            "\u001b[1m问题：小刚是小明的朋友，小明不喜欢小花。小花喜欢谁？\n",
            "答案：\u001b[0m小花喜欢小刚。\n",
            "\n",
            "English: \n",
            "CPU times: user 246 ms, sys: 582 ms, total: 828 ms\n",
            "Wall time: 803 ms\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "RzjhuvFYOtHt",
        "outputId": "48b958e0-7ad0-4bab-9eeb-ae2d6952c418"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"\n",
        "问题：小明和小花是朋友。但是小花更喜欢小凡。小明就很讨厌小凡，告诉小花其实小凡是牙签。请问谁是牙签？\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=10, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "问题：小明和小花是朋友。但是小花更喜欢小凡。小明就很讨厌小凡，告诉小花其实小凡是牙签。请问谁是牙签？\n",
            "答案：\u001b[0m小明。\n",
            "\n",
            "问�\n",
            "CPU times: user 186 ms, sys: 349 ms, total: 535 ms\n",
            "Wall time: 533 ms\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "O6YPPhrOWHOT",
        "outputId": "9f97f620-a080-49dc-b46c-88df756cc9c4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "context = \"\"\"问题：写一个java的测试文件来测试一个palindrome的算法\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=256, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：写一个java的测试文件来测试一个palindrome的算法\n",
            "答案：\u001b[0m\n",
            "\n",
            "1. 写一个测试文件来测试一个palindrome的算法\n",
            "\n",
            "2. 写一个测试文件来测试一个palindrome的算法\n",
            "\n",
            "3. 写一个测试文件来测试一个palindrome的算法\n",
            "\n",
            "4. 写一个测试文件来测试一个palindrome的算法\n",
            "\n",
            "5. 写一个测试文件来测试一个palindrome的算法\n",
            "\n",
            "6. 写一个测试文件来测试一个palindrome的算法\n",
            "\n",
            "7. 写一个测试文件来测试一个palindrome的算法\n",
            "\n",
            "8.\n",
            "CPU times: user 1.29 s, sys: 4.76 s, total: 6.05 s\n",
            "Wall time: 6.9 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "WnXWnaTNW_xy",
        "outputId": "e980566d-1b5b-4695-e60b-de7ee2006f36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context = \"\"\"问题：用python写一个程序，能得到输入的三个数中间的那个数字\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=256, context=context)[0])"
      ],
      "outputs": [],
      "metadata": {
        "id": "RtB1C7v_X2ey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context = \"\"\"问题：编写一个python程序从3个输入数字中获取第二大整数\n",
        "回答：```python\n",
        "def middle_number(a, b, c):\n",
        "    return sorted([a,b,c])[1]\n",
        "```\n",
        "\n",
        "问题：编写一个python程序，从6个输入数字中得到第4大的浮点数\n",
        "回答：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=64, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m问题：编写一个python程序从3个输入数字中获取第二大整数\n",
            "回答：```python\n",
            "def middle_number(a, b, c):\n",
            "    return sorted([a,b,c])[1]\n",
            "```\n",
            "\n",
            "问题：编写一个python程序，从6个输入数字中得到第4大的浮点数\n",
            "回答：\u001b[0m```python\n",
            "def middle_number(a, b, c, d, e, f):\n",
            "    return float(a+b+c+d+e+f)/6\n",
            "```\n",
            "\n",
            "问题：编写一个python�\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "ak_FU-pkdzqj",
        "outputId": "b438a0a8-bd0b-48d4-9fcd-24f497a4c91d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context = \"\"\"Question: write a python program to get 2nd largest integer from 3 input numbers\n",
        "Answer: ```python\n",
        "def middle_number(a, b, c):\n",
        "    return sorted([a,b,c])[1]\n",
        "```\n",
        "\n",
        "Question: write a python program to get the 4th largest float from 6 input numbers\n",
        "Answer: \n",
        "\"\"\"\n",
        "print(infer(top_p=0, temp=1.0, gen_len=64, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mQuestion: write a python program to get 2nd largest integer from 3 input numbers\n",
            "Answer: ```python\n",
            "def middle_number(a, b, c):\n",
            "    return sorted([a,b,c])[1]\n",
            "```\n",
            "\n",
            "Question: write a python program to get the 4th largest float from 6 input numbers\n",
            "Answer: \n",
            "\u001b[0m```python\n",
            "def middle_float(a, b, c, d, e, f):\n",
            "    return sorted([a,b,c,d,e,f])[1]\n",
            "```\n",
            "\n",
            "Question: write a python program to get the 5th largest integer from 6 input\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "blTk7eRYZPcH",
        "outputId": "3b0926c7-0aa2-4134-9abd-7f1ad264ce57"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context = \"\"\"问题：用python写一个程序，能得到输入的三个数排名第二的那个数字\n",
        "答案：```python\n",
        "def middle_number(a, b, c):\n",
        "    return sorted([a,b,c])[1]\n",
        "```\n",
        "\n",
        "问题：用python写一个程序，能得到输入的五个数中排名倒数第二的那个数字\n",
        "答案：\n",
        "\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=256, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 6.894907712936401s\n",
            "\u001b[1m问题：用python写一个程序，能得到输入的三个数排名第二的那个数字\n",
            "答案：```python\n",
            "def middle_number(a, b, c):\n",
            "    return sorted([a,b,c])[1]\n",
            "```\n",
            "\n",
            "问题：用python写一个程序，能得到输入的五个数中排名倒数第二的那个数字\n",
            "答案：\n",
            "\u001b[0m```python\n",
            "def middle_number(a, b, c, d, e):\n",
            "    return sorted([a,b,c,d,e])[1]\n",
            "```\n",
            "\n",
            "问题：用python写一个程序，能得到输入的三个数排名第二的那个数字\n",
            "答案：```python\n",
            "def middle_number(a, b, c):\n",
            "    return sorted([a,b,c])[1]\n",
            "```\n",
            "\n",
            "问题：用python写一个程序，能得到输入的三个数排名第二的那个数字\n",
            "答案：```python\n",
            "def middle_number(a, b, c):\n",
            "    return sorted([a,b,c])[1]\n",
            "```\n",
            "\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "ShZBnm1XV3Wg",
        "outputId": "bacfd6b6-3545-47bf-c81f-c24ead3c31b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context = \"\"\"问题：写一个SQL来选择一个名叫people的表格里面除了name字段的所有其他内容\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=256, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 6.893093109130859s\n",
            "\u001b[1m问题：写一个SQL来选择一个名叫people的表格里面除了name字段的所有其他内容\n",
            "答案：\u001b[0m\n",
            "SELECT * FROM people WHERE name = 'John'\n",
            "\n",
            "如果你想要查看所有人的名字，你可以写下这样：\n",
            "SELECT * FROM people\n",
            "\n",
            "如果你想要查看所有人的名字，你可以写下这样：\n",
            "SELECT * FROM people\n",
            "\n",
            "如果你想要查看所有人的名字，你可以写下这样：\n",
            "SELECT * FROM people\n",
            "\n",
            "如果你想要查看所有人的名字，你可以写下这样：\n",
            "SELECT * FROM people\n",
            "\n",
            "如果你想要查看所有人的名字，你\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "yF1m2IOQWpVj",
        "outputId": "e2a48549-d407-438e-e679-ae85d9dd1f3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context = \"\"\"问题：写一个SQL来选择一个名叫people的表格里面除了name字段的所有其他内容\n",
        "答案：SELECT * EXCEPT (name) FROM people\n",
        "\n",
        "问题：写一个SQL来选择一个名叫biz的表格里面除了price和quantity以外的所有其他内容，并限制在9月份\n",
        "答案：\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=256, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 6.895963430404663s\n",
            "\u001b[1m问题：写一个SQL来选择一个名叫people的表格里面除了name字段的所有其他内容\n",
            "答案：SELECT * EXCEPT (name) FROM people\n",
            "\n",
            "问题：写一个SQL来选择一个名叫biz的表格里面除了price和quantity以外的所有其他内容，并限制在9月份\n",
            "答案：\u001b[0mSELECT * FROM biz WHERE price < 9 AND quantity > 0\n",
            "\n",
            "问题：写一个SQL来选择一个名叫people的表格里面除了name字段的所有其他内容，并限制在9月份\n",
            "答案：SELECT * FROM people WHERE name NOT LIKE '%9%'\n",
            "\n",
            "问题：写一个SQL来选择一个名叫people的表格里面除了name字段的所有其他内容，并限制在9月份\n",
            "答案：SELECT * FROM people WHERE name NOT LIKE '%9%' AND name NOT LIKE '%9%'\n",
            "\n",
            "问题：写一个SQL来选择一个\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "3P3V7UlyXUAC",
        "outputId": "242841b3-acb6-4505-f45c-7c32673d3d9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "context=\"\"\"how to explain euler's formula?\"\"\"\n",
        "print(infer(top_p=0, temp=0, gen_len=128, context=context)[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completion done in 3.578883647918701s\n",
            "\u001b[1mhow to explain euler's formula?\u001b[0m\n",
            "\n",
            "I'm trying to understand Euler's formula for the number of partitions of a number.\n",
            "\n",
            "A:\n",
            "\n",
            "The formula is\n",
            "$$\\sum_{k=1}^n \\frac{1}{k} = \\log(n+1) + \\gamma + O(1/n)$$\n",
            "where $\\gamma$ is the Euler-Mascheroni constant.\n",
            "The proof is simple:\n",
            "$$\\sum_{k=1}^n \\frac{1}{k} = \\sum_{k=1}^n \\int_0^1 x^{k-1\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "R3oDY1Z9XfDV",
        "outputId": "5a8e0c70-31bb-44bf-9ca0-d1da64c0423e"
      }
    }
  ]
}